Abstract

Introduction
The rapid advancements in Artificial Intelligence(AI), particularly the emergence of Large Language Models (LLMs), have fundamentally reshaped the landscape of high-performance computing. LLMs, with their unprecedented scale and complexity, impose immense demands on computational throughput, memory capacity and bandwidth, and inter-processor communication. The training and inference of state-of-the-art LLMs often involve trillions of floating-point operations, terabytes of model parameters, and intricate communication patterns across thousands of accelerators.
Traditional chip architectures, typically relying on multi-chip packages or chiplet-based designs, are increasingly struggling to meet these escalating demands. The inherent limitations of inter-chip communication (e.g., I/O pin count, power consumption, latency) and the packaging overheads create bottlenecks that hinder the efficient scaling of LLM workloads. Wafer-scale integration (WSI) emerges as a promising paradigm to overcome these limitations by fabricating an entire system on a single silicon wafer, thereby enabling massive compute resources, ultra-high bandwidth, low-latency communication, and massive on-wafer memory integration.
However, designing wafer-scale chips presents formidable challenges, particularly in the realm of design space exploration (DSE). The design space for wafer-scale architectures is exceptionally vast. It encompasses a multitude of configurable parameters, including diverse types and configurations of compute cores (e.g., as explored in "Scale-Out Packageless Processing"), various memory hierarchies and capacities (e.g., different HBM configurations), and complex on-wafer communication network topologies. The sheer physical size of a wafer further exacerbates this combinatorial explosion, leading to an astronomical number of potential architectural configurations. Furthermore, evaluating each design point is computationally intensive. State-of-the-art simulators like Astra-sim 2.0, while offering high fidelity, can take upwards of 20 minutes for a single simulation run. Given that a typical digital chip design cycle spans approximately six months, exhaustively exploring such a vast design space through simulation is simply infeasible. Compounding this challenge, the manufacturing processes for wafer-scale chips are still in an exploratory phase, making each tape-out incredibly expensive (e.g., an 8-inch wafer-scale device, as seen in "8″ wafer-scale, centimeter-sized, high-efficiency metalenses in the ultraviolet"). Consequently, imprecise or sub-optimal designs can incur prohibitive costs and significant delays. Therefore, there is a critical need for DSE methodologies that are both highly accurate and remarkably fast.
Existing simulation tools, while valuable for architectural performance evaluation, often present a trade-off between speed and accuracy. These can be broadly categorized:
•	Analytical Models: Simulators like Astra-sim, Astra-sim 2.0 (for initial estimations), vtrain (which scales small-scale accurate simulations to full systems), and AMPed offer high simulation speed by abstracting away fine-grained details. While fast, their accuracy can be limited, especially for complex interactions and heterogeneous architectures.
•	Event-Driven Simulators: Tools such as PALM and ns3 (which serves as a backend for Astra-sim 2.0) provide a more detailed view by modeling discrete events. They offer better accuracy than analytical models but are inherently slower.
•	Cycle-Accurate Simulators: Muchisim (a multi-threaded simulator), GeneSys (the backend for LLMServingSim), and Garnet (a backend for Astra-sim) provide the highest fidelity by modeling system behavior cycle-by-cycle. While indispensable for precise performance analysis, their execution times are significantly longer, making them impractical for broad DSE.
•	Other Custom Solutions (Machine Learning-based): Approaches like GPU-Scale-out-model and Vidur leverage machine learning to predict performance. While potentially fast after training, they rely heavily on the quality and coverage of training data and may struggle with generalization to unseen architectural configurations.
Current DSE methods applied to wafer-scale chip design often resort to limiting the number of combinations to shrink the search space or accepting sub-optimal solutions to reduce search time. For instance, the "Scale-Out Packageless Processing" work restricts its exploration to a simplified architecture consisting of a single compute core and a unified L1/L2 memory unit per tile on the wafer. While this simplifies the problem, it severely constrains the architectural diversity, preventing the exploration of more complex and potentially more efficient compute-to-memory ratios and heterogeneous configurations that our method can explore. Similarly, Theseus controls search time by limiting the number of iterations, which, while practical, cannot guarantee the discovery of the optimal solution or provide a confidence level regarding the quality of the found solution. These limitations highlight a significant gap in the current design methodology for wafer-scale systems.
In this paper, we propose a novel DSE framework specifically tailored for wafer-scale architectures, addressing the aforementioned challenges through three key innovations:
•	Comprehensive Wafer-Scale Architecture Design Space: We establish a formalized and expansive design space for wafer-scale architectures, encompassing a rich variety of compute cores, memory units, and communication units, while rigorously incorporating physical constraints inherent to wafer-scale integration. This allows for a much broader and more realistic exploration than prior works.
•	Quantifying Optimal Solution Confidence: We introduce a novel methodology to establish a direct link between the accuracy of the simulation models used during DSE and the confidence level of identifying the true optimal architectural solution. This provides designers with a quantifiable measure of certainty in their design choices.
•	Hybrid Simulation System for Guaranteed Optimality: We develop a sophisticated hybrid simulation system that combines coarse-grained, high-speed simulation with fine-grained, high-accuracy simulation. This system is designed to efficiently prune the vast design space while guaranteeing the identification of the optimal solution. In scenarios where an absolute guarantee of optimality cannot be provided due to practical constraints, our system is capable of providing a precise confidence level for the best-found solution, empowering designers with informed decision-making.




Background

The rapid evolution of Artificial Intelligence, particularly the emergence of Large Language Models (LLMs), has fundamentally reshaped the landscape of high-performance computing. As demonstrated by [Gopher, 2021], increasing the scale of LLMs directly correlates with significant improvements in their performance and capabilities. This insight has driven an unprecedented scaling of model sizes in recent years, evolving from the 345 million parameters of the original BERT model [BERT, 2019] to the 8.3 billion parameters of Megatron-LM [Megatron-LM, 2022] and the staggering 1.8 trillion parameters of GPT-4 [GPT-4, 2023]. This exponential growth in model complexity translates directly into an insatiable demand for computational throughput, memory capacity, and communication bandwidth, particularly during the training phase. Traditional accelerators, primarily Graphics Processing Units (GPUs), are increasingly pushed to their reticle limits—the maximum size a chip can be manufactured on a single photolithography mask. To circumvent these physical constraints, advanced GPU designs, such as NVIDIA's GB200 [NVIDIA, 2024], have adopted innovative approaches like high-bandwidth interconnections to link multiple compute dies, effectively creating larger, more powerful systems that surpass the traditional reticle boundary.

To address the escalating demands of LLMs and overcome the inherent limitations of multi-chip packaging, wafer-scale integration (WSI) has emerged as a compelling architectural paradigm. WSI involves fabricating an entire system, or a significant portion thereof, on a single silicon wafer, thereby eliminating the need for traditional packaging and enabling unprecedented levels of on-chip communication bandwidth and low latency. Pioneering efforts in this domain include the Cerebras Wafer-Scale Engine (WSE) [Cerebras, 2020], which integrates hundreds of thousands of AI-optimized cores and gigabytes of on-chip memory onto a single wafer, specifically designed for large-scale deep learning workloads. Similarly, Tesla's Dojo D1 chip [Tesla, 2021] employs a wafer-scale approach to create a highly interconnected compute fabric for AI training. Academic research, such as that from UCLA [UCLA, 202X], has also significantly contributed to the understanding and development of wafer-scale architectures, exploring novel interconnects, memory hierarchies, and fault tolerance mechanisms crucial for the viability of such large-scale systems.

The immense scale of wafer-scale integration, while offering unparalleled performance potential, simultaneously introduces an extraordinarily vast and complex design space. This complexity renders traditional exhaustive design space exploration (DSE) methods, which rely on enumerating and simulating every possible configuration, utterly impractical. Consider the architectural parameters explored in works like "Scale-Out Packageless Processing" [Scale-Out, 202X], which might involve 128 types of compute cores, $M$ different HBM configurations, and $N$ distinct Network-on-Chip (NoC) topologies. If each combination of these high-level parameters defines a 'solution space' containing, for instance, 32 specific architectural instantiations, the total number of architectures to evaluate would be $128 \times M \times N \times 32$. If each detailed simulation takes approximately 2 hours, the total time required for exhaustive exploration would be $8192 \times M \times N$ hours. For realistic values of $M$ and $N$ (e.g., $M=4, N=4$), this translates to over 130,000 hours, or more than 15 years on a single machine, clearly demonstrating the infeasibility of such an approach.

Our proposed methodology fundamentally transforms this challenge. By employing a multi-fidelity simulation approach, our initial coarse-grained evaluation of each design point can be completed in mere seconds. This allows us to rapidly screen the entire $128 \times M \times N$ high-level design space in approximately $128 \times M \times N$ seconds, identifying a significantly reduced subset of promising candidates—estimated to be around $100 \times M \times N$ potential optimal solutions. Even if each of these refined candidates then requires a detailed 2-hour simulation, executing this on an 8-card compute cluster would reduce the total detailed evaluation time to approximately $25 \times M \times N$ days. This dramatic reduction in exploration time, from years to days, underscores the critical importance of our fast and accurate DSE framework for the practical development of next-generation wafer-scale AI accelerators.
Motivations and Insights
The design of complex hardware systems, particularly at the scale of wafer-scale integration, critically relies on accurate performance prediction through simulation. However, a fundamental limitation of current architectural simulators is that their chosen level of accuracy is often determined heuristically. This empirical selection inherently means that there is no formal guarantee that the identified "optimal" design point is indeed the true global optimum. Furthermore, these simulators typically do not provide a quantifiable confidence level for the optimality of the chosen solution. For wafer-scale chips, where development costs are astronomical and fabrication cycles are lengthy, an imprecise or sub-optimal design choice can lead to catastrophic financial and temporal setbacks. This lack of guaranteed optimality and confidence is a significant impediment to efficient and reliable wafer-scale system design.
Addressing this gap necessitates overcoming several profound challenges in the realm of design space exploration (DSE) for wafer-scale architectures:
1. Defining and Navigating the Expansive Design Space:
The first challenge lies in formally defining the vast and intricate design space of wafer-scale architectures. This space is characterized by a multitude of configurable parameters, including diverse specifications for compute cores, memory units (e.g., HBM configurations, on-wafer caches), and communication units (e.g., Network-on-Chip topologies). Crucially, this definition must rigorously incorporate physical constraints (e.g., power density, thermal dissipation, routing congestion) and manufacturing process limitations inherent to wafer-scale integration. An exhaustive enumeration of this space is computationally intractable, as demonstrated in the Introduction. Moreover, a simple brute-force search would yield a multitude of sub-optimal solutions. Therefore, a critical requirement is to develop effective methodologies for pruning this immense space, efficiently identifying the Pareto optimal solutions that represent the best trade-offs across multiple performance metrics (e.g., throughput, latency, power efficiency).
2. Quantifying the Relationship Between Simulation Error and Design Accuracy:
A second, equally critical challenge is to precisely understand and quantify the relationship between the inherent error in simulation models and the accuracy of the final design choice. Given a true optimal solution within a specific design subspace, how do varying levels of simulation error (e.g., from coarse-grained analytical models versus cycle-accurate simulators) influence the set of "potentially optimal" solutions identified? This involves developing a robust framework to analyze how simulation inaccuracies propagate through the DSE process and affect the confidence in selecting the true optimum. Without such an understanding, designers are left to make decisions based on potentially misleading performance estimates.
Our Proposed Design Methodology:
To address these challenges, we propose a novel, multi-fidelity DSE framework for wafer-scale architectures. Our methodology is built upon the following key insights:
•	Hybrid Multi-Fidelity Simulation: We employ a two-stage simulation approach. Initially, we utilize highly efficient analytical models for a coarse-grained screening of the entire design space. This rapid initial pass is designed not only to identify a preliminary "best" candidate but, more importantly, to efficiently prune the vast space and retain all potentially optimal solutions within a defined error margin. Subsequently, a more precise, fine-grained simulator (such as Astra-sim) is employed to thoroughly evaluate this significantly reduced set of promising candidates, thereby pinpointing the true optimal solution with high fidelity.
•	Error Tolerance and Guaranteed Optimality: Through a rigorous analysis of the design space characteristics and the properties of our analytical models, we can derive an "error tolerance" threshold. Within this tolerance, our coarse-grained simulation is sufficient to uniquely identify the optimal solution, significantly accelerating the DSE process.
•	Configurable Accuracy and Confidence Quantification: Simulators like Astra-sim offer configurable levels of accuracy, allowing us to select the appropriate fidelity based on the specific requirements of the design stage. Crucially, even in scenarios where the chosen simulation fidelity (due to computational budget or inherent model limitations) cannot definitively narrow down to a single optimal solution, our framework is designed to provide a quantifiable confidence level for the best-found solution among the remaining candidates. This empowers designers with informed decision-making, even when absolute certainty is unattainable, by providing a probabilistic measure of optimality.
By integrating these innovations, our framework aims to provide a DSE methodology that is both orders of magnitude faster than traditional exhaustive simulation and capable of delivering high-confidence optimal designs for the next generation of wafer-scale AI accelerators.

Methods
Methods
Our proposed methodology for wafer-scale architecture design space exploration (DSE) is structured around two core components: first, the systematic construction and pruning of the vast design space, and second, the establishment of a quantifiable relationship between simulation accuracy and the confidence in identifying the optimal solution.
1. Establishing the Design Space
We begin by defining the fundamental building block of our wafer-scale architecture: a homogeneous die. Each die is conceptualized as a central compute core surrounded by configurable memory and communication units. The overall wafer architecture is then formed by tiling these identical dies across the wafer surface. A critical physical constraint is that the dimensions of an individual die directly dictate the total number of dies that can be integrated onto a single wafer, thereby influencing the aggregate performance.
1.1 Deduplication of Architecturally Equivalent Designs
The combinatorial nature of arranging memory and communication units around a compute core leads to a massive number of potential configurations. A significant challenge is the presence of architecturally equivalent designs that, despite having different textual representations, exhibit identical performance characteristics due to symmetry or functional equivalence. For instance, consider the arrangement of High Bandwidth Memory (HBM) and Network-on-Chip (NoC) units along one edge of a compute core. The sequence HBM-NOC-NOC-HBM might yield identical performance to NOC-HBM-HBM-NOC if the underlying communication and memory access patterns are symmetric or if the specific placement within the sequence does not impact critical path latencies or bandwidths.
To mitigate the exponential growth of the design space, we implement a rigorous deduplication process. This process is applied early in the exploration, specifically when generating permutations for a single edge of the compute core. By identifying and removing these redundant designs at the lowest level of granularity, we prevent their combinatorial explosion when these single-edge configurations are combined to form complete die designs. This proactive deduplication significantly reduces the search space and computational overhead for subsequent evaluation stages.
1.2 Pruning of Sub-optimal Designs
The design of each die involves a fundamental trade-off: increasing the number or capacity of memory and communication units along a die's edge generally enhances the performance of that individual die. However, this also inevitably increases the die's physical dimensions. A larger die size means fewer dies can be integrated onto a fixed-size wafer, potentially leading to a decrease in overall wafer-level performance. To navigate this complex trade-off and efficiently narrow down the design space, we employ a two-stage pruning strategy based on Pareto optimality principles:
(1) Edge-Level Pruning: During the exploration of possible arrangements for a single edge of the compute core, we apply a localized pruning rule. If two distinct edge configurations, say Configuration A and Configuration B, result in the same increase in die size (i.e., they occupy an equivalent physical footprint), but Configuration A consistently provides inferior performance improvement compared to Configuration B across relevant metrics (e.g., bandwidth, latency), then Configuration A is immediately pruned from the design space. This ensures that for any given physical footprint, only the most performant edge configurations are retained.
(2) Wafer-Level Pruning: After generating complete wafer architectures by combining valid die designs, we perform a higher-level pruning step. If a complete wafer architecture, Wafer A, is found to be strictly dominated by another wafer architecture, Wafer B, across all critical performance metrics—specifically, if Wafer A exhibits lower aggregate compute throughput, lower total memory bandwidth, and inferior communication performance compared to Wafer B—then Wafer A is deemed sub-optimal and removed from further consideration. This multi-objective pruning ensures that only Pareto-optimal wafer designs, representing the best trade-offs across the entire system, are carried forward.
2. Error Threshold and Accuracy-Confidence Relationship
A cornerstone of our methodology is the explicit quantification of the relationship between the accuracy of our simulation models and the confidence in identifying the true optimal design. We model this relationship using an S-shaped (Sigmoid) curve, which illustrates how the number of "potentially optimal" solutions (or the uncertainty in identifying the true optimum) changes as simulation accuracy decreases.
•	First Segment (High Accuracy Plateau): In this initial segment of the S-curve, the simulation accuracy is very high. Within this range, even minor reductions in accuracy do not affect the identification of the true optimal solution. This represents the "lossless accuracy range," where the lowest possible accuracy within this segment is sufficient to guarantee the selection of the precise optimal design.
•	Second Segment (Steep Slope): As simulation accuracy begins to decrease beyond the first segment, the number of potentially optimal solutions (i.e., designs that appear optimal under the given accuracy level but might not be the true optimum) starts to increase rapidly. This segment represents the critical trade-off zone where designers must balance simulation speed (achieved by lower accuracy) against the increasing uncertainty in identifying the absolute best design. Our framework quantifies this uncertainty, providing a confidence level for the best-found solution at any given accuracy point.
•	Third Segment (Low Accuracy Plateau): In this final segment, further reductions in simulation accuracy yield diminishing returns in terms of increased simulation speed, while the number of potentially optimal solutions (or the uncertainty) has largely saturated. This indicates that the simulation has become too coarse-grained to provide meaningful differentiation among designs, and the confidence in identifying the true optimum is very low.
Based on this S-curve analysis, we can identify several "suitable accuracy points" for DSE:
(1) Intersection of the First and Second Segments: This point represents the minimum simulation accuracy required to guarantee the identification of the true optimal design. By operating at this point, designers can achieve the most efficient DSE while ensuring absolute design accuracy.
(2) Any Point on the Second Segment: For scenarios with tighter time budgets, designers can choose a point within this segment. While not guaranteeing the absolute optimum, our framework will provide a quantifiable confidence level for the best-found solution, allowing for an informed trade-off between simulation speed and design certainty.
(3) End of the Third Segment: This point marks the practical lower bound for useful simulation accuracy. Beyond this, the simulation provides little to no valuable information for design differentiation, and further reductions in accuracy offer negligible benefits in terms of speed.
Our DSE framework leverages this understanding by employing a multi-fidelity simulation approach. We use fast analytical models for initial coarse-grained screening, which operates at an accuracy level typically within the second or third segment of the S-curve. This allows us to rapidly prune the vast design space and identify a reduced set of promising candidates, including all potentially optimal solutions within a defined error tolerance. Subsequently, we employ a more precise simulator (e.g., Astra-sim, whose accuracy is configurable) to evaluate these candidates. If Astra-sim can be configured to operate at an accuracy corresponding to the first segment, we can definitively identify the true optimal solution. If computational constraints necessitate operating at a lower accuracy (within the second segment), our framework will provide a confidence level for the best-found solution, empowering designers with probabilistic guarantees.

System Implementation

Experiments

Related Works
